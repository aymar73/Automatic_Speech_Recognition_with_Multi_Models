{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf #version == 1.2\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import cells\n",
    "from models import Stack_Layers_Model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "############## Read data ##################\n",
    "train_feature_dir = './data/TIMIT/phn/train/mfcc/'\n",
    "train_label_dir = './data/TIMIT/phn/train/label/'\n",
    "test_feature_dir = './data/TIMIT/phn/test/mfcc/'\n",
    "test_label_dir = './data/TIMIT/phn/test/label/'\n",
    "# read data from local path \n",
    "# a list of feature, each one has shape [feature_num, time_step]\n",
    "train_feature_list = read_ndarray_from(train_feature_dir)\n",
    "# a list of label, each one has shape [label_num]\n",
    "train_label_list = read_ndarray_from(train_label_dir)\n",
    "test_feature_list = read_ndarray_from(test_feature_dir)\n",
    "test_label_list = read_ndarray_from(test_label_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the train feature, type: <type 'list'>, length: 100, type of each element: <type 'numpy.ndarray'>\n",
      "for the train label, type: <type 'list'>, length: 100, type of each element: <type 'numpy.ndarray'>\n",
      "for the test feature, type: <type 'list'>, length: 100, type of each element: <type 'numpy.ndarray'>\n",
      "for the test label, type: <type 'list'>, length: 100, type of each element: <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "############ have a glance of the preprocessed data ############\n",
    "\n",
    "# only pick a part of the dataset for quick debug\n",
    "train_feature_list = train_feature_list[:100]\n",
    "train_label_list = train_label_list[:100]\n",
    "test_feature_list = test_feature_list[:100]\n",
    "test_label_list = test_label_list[:100]\n",
    "\n",
    "print(\"for the train feature, type: {}, length: {}, type of each element: {}\".format(type(train_feature_list), len(train_feature_list), type(train_feature_list[0])))\n",
    "print(\"for the train label, type: {}, length: {}, type of each element: {}\".format(type(train_label_list), len(train_label_list), type(train_label_list[0])))\n",
    "print(\"for the test feature, type: {}, length: {}, type of each element: {}\".format(type(test_feature_list), len(test_feature_list), type(test_feature_list[0])))\n",
    "print(\"for the test label, type: {}, length: {}, type of each element: {}\".format(type(test_label_list), len(test_label_list), type(test_label_list[0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "################### Define hyper-paramaters ########################\n",
    "class Argument(object):\n",
    "    def __init__(self):\n",
    "        self.max_epoch = 50\n",
    "        self.num_layer = 2\n",
    "        self.num_hidden = 128\n",
    "        self.num_featue = train_feature_list[0].shape[0]\n",
    "        self.num_class = 62       \n",
    "        \n",
    "        self.lr_rate = 0.001\n",
    "        self.batch_size = 32\n",
    "        self.max_timestep = get_max_timestep(train_feature_list, test_feature_list)\n",
    "        self.layer_norm = True\n",
    "        self.dropout = 0.1 \n",
    "        \n",
    "        self.cell_type = 'LSTMCell' #option: LSTMCell, RNNCell, GRUCell\n",
    "        self.model_type = 'bidirection' #option: unidirection, bidirection, resnet, highway, seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "################### Build a model #################################\n",
    "args = Argument()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #define input and output tensor\n",
    "    inputs = tf.placeholder(dtype=tf.float32, shape=[args.batch_size, args.max_timestep, args.num_featue], name=\"intputs\")\n",
    "    targets_idx = tf.placeholder(tf.int64)\n",
    "    targets_val = tf.placeholder(tf.int32)\n",
    "    targets_shape = tf.placeholder(tf.int64)\n",
    "    targets = tf.SparseTensor(targets_idx, targets_val, targets_shape)\n",
    "    seq_len = tf.placeholder(tf.int32, [args.batch_size], name=\"seq_len\")\n",
    "    #stack multi-layer networks\n",
    "    layers_model = Stack_Layers_Model(args, inputs, targets, seq_len)\n",
    "    logits = layers_model.build_model() #the logits is a tensor with shape: [batch_size, max_timestep, num_class]\n",
    "    \n",
    "    logits = tf.transpose(logits, [1,0,2]) #time major, shape: [max_timestep, batch_size, num_class]\n",
    "    \n",
    "    \n",
    "    #optimizer\n",
    "    loss = tf.nn.ctc_loss(targets, logits, seq_len)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(args.lr_rate).minimize(cost)\n",
    "#     predictions = tf.to_int32(\n",
    "#                 tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)[0][0])\n",
    "    predictions = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)[0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"transpose:0\", shape=(686, 32, 62), dtype=float32)\n",
      "SparseTensor(indices=Tensor(\"CTCBeamSearchDecoder:0\", shape=(?, 2), dtype=int64), values=Tensor(\"CTCBeamSearchDecoder:1\", shape=(?,), dtype=int64), dense_shape=Tensor(\"CTCBeamSearchDecoder:2\", shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(logits)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in train mode, epoch: 1/50, PER: 2.51, time: 33.30s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (32, 613, 39) for Tensor u'intputs:0', which has shape '(32, 686, 39)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d6879414b9ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_targets_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_val\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_targets_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 targets_shape: test_targets_shp, seq_len: test_seq_len}\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mtest_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mtest_batch_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_edit_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhaoyu106/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhaoyu106/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    973\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    976\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (32, 613, 39) for Tensor u'intputs:0', which has shape '(32, 686, 39)'"
     ]
    }
   ],
   "source": [
    "############# Start Trainging ####################\n",
    "batch_size = args.batch_size\n",
    "max_epoch = args.max_epoch\n",
    "\n",
    "#split dataset into serveral batches\n",
    "level = 'phn'\n",
    "\n",
    "(train_batch_list, _) = data_lists_to_batches(train_feature_list, train_label_list, batch_size, level)\n",
    "num_train_batch = len(train_batch_list)\n",
    "train_error_list = [] #define a list to store each epoch error in training\n",
    "\n",
    "(test_batch_list, _) = data_lists_to_batches(test_feature_list, test_label_list, batch_size, level)\n",
    "num_test_batch = len(test_batch_list)\n",
    "test_error_list = [] #define a list to store each epoch error in testing\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(max_epoch):\n",
    "        start_time = time.time()\n",
    "        train_batch_error = np.zeros(num_train_batch) #define a ndarray to store each batch error\n",
    "        #train the neural network\n",
    "        for i in range(num_train_batch):\n",
    "            train_inputs, train_targets, train_seq_len = train_batch_list[i]\n",
    "            train_targets_idx, train_targets_val, train_targets_shp = train_targets\n",
    "            feed_dict = {\n",
    "                inputs: train_inputs, targets_idx: train_targets_idx, targets_val: train_targets_val,\n",
    "                targets_shape: train_targets_shp, seq_len: train_seq_len}\n",
    "            _, train_cost, train_preditions, train_targets = sess.run([optimizer, cost, predictions, targets], \n",
    "                                                                      feed_dict=feed_dict)\n",
    "            train_batch_error[i] = get_edit_distance([train_preditions.values], [train_targets.values], True, level)\n",
    "        \n",
    "        epoch_error = np.mean(train_batch_error) #calculate the mean value of batches in specify epoch\n",
    "        end_time = time.time()\n",
    "        print(\"in train mode, epoch: {}/{}, PER: {:.2f}, time: {:.2f}s\".format(epoch+1, max_epoch, epoch_error, end_time-start_time))\n",
    "        train_error_list.append(epoch_error)\n",
    "        \n",
    "        #test the neural network\n",
    "        test_batch_error = np.zeros(num_test_batch) #define a ndarray to store each batch error\n",
    "        for i in range(num_test_batch):\n",
    "            test_inputs, test_targets, test_seq_len = test_batch_list[i]\n",
    "            test_targets_idx, test_targets_val, test_targets_shp = test_targets\n",
    "            feed_dict = {\n",
    "                inputs: test_inputs, targets_idx: test_targets_idx, targets_val: test_targets_val,\n",
    "                targets_shape: test_targets_shp, seq_len: test_seq_len}\n",
    "            test_cost, test_predictions, test_targets = sess.run([cost, predictions, targets], feed_dict=feed_dict)\n",
    "            \n",
    "            test_batch_error[i] = get_edit_distance([test_predictions.values], [test_targets.values], True, level)\n",
    "            \n",
    "        epoch_error = np.mean(test_batch_error) #calculate the mean value of batches in specify epoch\n",
    "        print(\"in test mode, epoch: {}/{}, PER: {:.2f}\".format(epoch+1, max_epoch, epoch_error))\n",
    "        test_error_list.append(epoch_error)\n",
    "        print(\"test truth:\\n\"+output_to_sequence(test_targets))\n",
    "        print(\"test prediction:\\n\"+output_to_sequence(test_predictions))\n",
    "        \n",
    "        #store trained model and logs\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
