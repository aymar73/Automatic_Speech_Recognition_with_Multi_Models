{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf #version == 1.2\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import cells\n",
    "from models import Stack_Layers_Model\n",
    "import time\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "############## Read data ##################\n",
    "train_feature_dir = './data/TIMIT/phn/train/mfcc/'\n",
    "train_label_dir = './data/TIMIT/phn/train/label/'\n",
    "test_feature_dir = './data/TIMIT/phn/test/mfcc/'\n",
    "test_label_dir = './data/TIMIT/phn/test/label/'\n",
    "# read data from local path \n",
    "# a list of feature, each one has shape [feature_num, time_step]\n",
    "train_feature_list = read_ndarray_from(train_feature_dir)\n",
    "# a list of label, each one has shape [label_num]\n",
    "train_label_list = read_ndarray_from(train_label_dir)\n",
    "test_feature_list = read_ndarray_from(test_feature_dir)\n",
    "test_label_list = read_ndarray_from(test_label_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the train feature, type: <type 'list'>, length: 200, type of each element: <type 'numpy.ndarray'>\n",
      "for the train label, type: <type 'list'>, length: 200, type of each element: <type 'numpy.ndarray'>\n",
      "for the test feature, type: <type 'list'>, length: 100, type of each element: <type 'numpy.ndarray'>\n",
      "for the test label, type: <type 'list'>, length: 100, type of each element: <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "############ have a glance of the preprocessed data ############\n",
    "\n",
    "# # only pick a part of the dataset for quick debug\n",
    "train_feature_list = train_feature_list[:200]\n",
    "train_label_list = train_label_list[:200]\n",
    "test_feature_list = test_feature_list[:100]\n",
    "test_label_list = test_label_list[:100]\n",
    "\n",
    "print(\"for the train feature, type: {}, length: {}, type of each element: {}\".format(type(train_feature_list), len(train_feature_list), type(train_feature_list[0])))\n",
    "print(\"for the train label, type: {}, length: {}, type of each element: {}\".format(type(train_label_list), len(train_label_list), type(train_label_list[0])))\n",
    "print(\"for the test feature, type: {}, length: {}, type of each element: {}\".format(type(test_feature_list), len(test_feature_list), type(test_feature_list[0])))\n",
    "print(\"for the test label, type: {}, length: {}, type of each element: {}\".format(type(test_label_list), len(test_label_list), type(test_label_list[0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "################### Define hyper-paramaters ########################\n",
    "class Argument(object):\n",
    "    def __init__(self):\n",
    "        self.max_epoch = 500\n",
    "        self.num_layer = 2\n",
    "        self.num_hidden = 128\n",
    "        self.num_featue = train_feature_list[0].shape[0]\n",
    "        self.num_class = 62       \n",
    "\n",
    "        self.lr_rate = 0.001\n",
    "        self.batch_size = 32\n",
    "        self.max_timestep = get_max_timestep(train_feature_list, test_feature_list)\n",
    "        self.layer_norm = True #only available for LSTMCell\n",
    "        self.dropout = 0.1 # drouput is only used between input and the first hidden layer, \n",
    "                           # and last hidden layer and output layer\n",
    "        self.isTrain = True #set a flag to judge whether is training, used for dropout\n",
    "        self.isBiRNN = True # set a flag to choose unidirectional or bidirectional RNN\n",
    "        self.isResNet = True #set a flag to chose whether use ressidual connection\n",
    "        self.cell_type = 'LSTMCell' #option: LSTMCell, RNNCell, GRUCell, HyperLSTMCell\n",
    "        \n",
    "        self.model_dir = \"./model/\" #file path to store trained model\n",
    "        self.log_dir = \"./log/\" #file path to store logs\n",
    "        self.log_name = str(datetime.datetime.now())+'.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we highly recommend to use ResNet when hidden layer is larger than 2\n"
     ]
    }
   ],
   "source": [
    "################### Build a model #################################\n",
    "args = Argument()\n",
    "#define input and output tensor\n",
    "\n",
    "inputs = tf.placeholder(dtype=tf.float32, shape=[args.batch_size, args.max_timestep, args.num_featue], name=\"intputs\")\n",
    "targets_idx = tf.placeholder(tf.int64)\n",
    "targets_val = tf.placeholder(tf.int32)\n",
    "targets_shape = tf.placeholder(tf.int64)\n",
    "targets = tf.SparseTensor(targets_idx, targets_val, targets_shape)\n",
    "seq_len = tf.placeholder(tf.int32, [args.batch_size], name=\"seq_len\")\n",
    "#stack multi-layer networks\n",
    "layers_model = Stack_Layers_Model(args, inputs, targets, seq_len)\n",
    "logits = layers_model.stack_RNN() #the logits is a tensor with shape: [batch_size, max_timestep, num_class]\n",
    "\n",
    "logits = tf.transpose(logits, [1,0,2]) #time major, shape: [max_timestep, batch_size, num_class]\n",
    "\n",
    "\n",
    "#optimizer\n",
    "loss = tf.nn.ctc_loss(targets, logits, seq_len)\n",
    "cost = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(args.lr_rate).minimize(cost)\n",
    "predictions = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of trainable params is 446270\n",
      "during training mode, batch: 1/6, epoch: 1, PER: 3.66216206551\n",
      "during training mode, batch: 2/6, epoch: 1, PER: 3.1961414814\n",
      "during training mode, batch: 3/6, epoch: 1, PER: 2.25117015839\n"
     ]
    }
   ],
   "source": [
    "############# Start Trainging ####################\n",
    "batch_size = args.batch_size\n",
    "max_epoch = args.max_epoch\n",
    "\n",
    "#split dataset into serveral batches\n",
    "level = 'phn'\n",
    "\n",
    "# feature_list = train_feature_list + test_feature_list\n",
    "# label_list = train_label_list + test_label_list\n",
    "\n",
    "\n",
    "# (batch_list, _) = data_lists_to_batches(feature_list, label_list, batch_size, level)\n",
    "# train_batch_list = batch_list[:len(train_feature_list)]\n",
    "# test_batch_list = batch_list[len(test_feature_list):]\n",
    "\n",
    "# num_train_batch = len(train_batch_list)\n",
    "# num_test_batch = len(test_batch_list)\n",
    "# train_error_list = [] #define a list to store each epoch error in training\n",
    "# test_error_list = [] #define a list to store each epoch error in testing\n",
    "\n",
    "\n",
    "(train_batch_list, _) = data_lists_to_batches(train_feature_list, train_label_list, batch_size, level, args.max_timestep)\n",
    "num_train_batch = len(train_batch_list)\n",
    "train_error_list = [] #define a list to store each epoch error in training\n",
    "\n",
    "(test_batch_list, _) = data_lists_to_batches(test_feature_list, test_label_list, batch_size, level, args.max_timestep)\n",
    "num_test_batch = len(test_batch_list)\n",
    "test_error_list = [] #define a list to store each epoch error in testing\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    num_trainable_params = np.sum([np.prod(v.shape) for v in tf.trainable_variables()]) #count params\n",
    "    print(\"num of trainable params is {}\".format(num_trainable_params))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(max_epoch):\n",
    "        #train the neural network\n",
    "        start_time = time.time()\n",
    "        train_batch_error = np.zeros(num_train_batch) #define a ndarray to store each batch error\n",
    "        args.isTrain = True\n",
    "        for i in range(num_train_batch):\n",
    "            train_inputs, train_targets, train_seq_len = train_batch_list[i]\n",
    "            train_targets_idx, train_targets_val, train_targets_shp = train_targets\n",
    "            feed_dict = {\n",
    "                inputs: train_inputs, targets_idx: train_targets_idx, targets_val: train_targets_val,\n",
    "                targets_shape: train_targets_shp, seq_len: train_seq_len}\n",
    "            _, train_cost, train_preditions, train_targets = sess.run([optimizer, cost, predictions, targets], \n",
    "                                                                      feed_dict=feed_dict)\n",
    "            train_batch_error[i] = get_edit_distance([train_preditions.values], [train_targets.values], True, level)\n",
    "            print(\"during training mode, batch: {}/{}, epoch: {}, PER: {}\".\\\n",
    "                  format(i+1, num_train_batch, epoch+1, train_batch_error[i]))\n",
    "            \n",
    "        train_epoch_error = np.mean(train_batch_error) #calculate the mean value of batches in specify epoch\n",
    "        end_time = time.time()\n",
    "        train_time = end_time - start_time\n",
    "        print(\"in train mode, epoch: {}/{}, PER: {:.2f}, time: {:.2f}s\".\\\n",
    "              format(epoch+1, max_epoch, train_epoch_error, train_time))\n",
    "        train_error_list.append(train_epoch_error)\n",
    "        \n",
    "        #test the neural network\n",
    "        test_batch_error = np.zeros(num_test_batch) #define a ndarray to store each batch error\n",
    "        start_time = time.time()\n",
    "        args.isTrain = False\n",
    "        for i in range(num_test_batch):\n",
    "            test_inputs, test_targets, test_seq_len = test_batch_list[i]            \n",
    "            test_targets_idx, test_targets_val, test_targets_shp = test_targets\n",
    "            feed_dict = {\n",
    "                inputs: test_inputs, targets_idx: test_targets_idx, targets_val: test_targets_val,\n",
    "                targets_shape: test_targets_shp, seq_len: test_seq_len}\n",
    "            test_cost, test_predictions, test_targets = sess.run([cost, predictions, targets], feed_dict=feed_dict)\n",
    "            test_batch_error[i] = get_edit_distance([test_predictions.values], [test_targets.values], True, level)\n",
    "            \n",
    "        test_epoch_error = np.mean(test_batch_error) #calculate the mean value of batches in specify epoch\n",
    "        end_time = time.time()\n",
    "        test_time = end_time - start_time\n",
    "        print(\"in test mode, epoch: {}/{}, PER: {:.2f}, time: {:.2f}s\".format(epoch+1, max_epoch, test_epoch_error, test_time))\n",
    "        test_error_list.append(test_epoch_error)\n",
    "        print(\"test truth:\\n\"+output_to_sequence(test_targets))\n",
    "        print(\"test prediction:\\n\"+output_to_sequence(test_predictions))\n",
    "\n",
    "        ################ save model and log info ##################\n",
    "        \n",
    "        #store trained model and logs\n",
    "        model_dir = args.model_dir\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.mkdir(model_dir) #create a new folder if not exist\n",
    "        check_point_path = os.path.join(model_dir, \"model.ckpt\")\n",
    "        saver.save(sess, check_point_path, global_step=epoch)\n",
    "        print(\"Model has been saved in {}\".format(model_dir))\n",
    "        #store logs in local file\n",
    "        log_dir = args.log_dir\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.mkdir(log_dir) #create a new log directionary if not exist\n",
    "        #open a log file\n",
    "        log_name = args.log_name\n",
    "        with open(os.path.join(log_dir, log_name), 'a') as f:\n",
    "            f.write(\"for train mode, epoch: {}, PER: {}, run time: {}\\n\".format(epoch+1, train_epoch_error, train_time))\n",
    "            f.write(\"for test mode, epoch: {}, PER: {}, run time: {}\\n\".format(epoch+1, test_epoch_error, test_time))\n",
    "            f.write(\"=================================================\\n\")\n",
    "            print(\"log has been saved in {}\".format(log_name))\n",
    "        print(\"===========================================\")\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
